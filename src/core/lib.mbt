// Generic parser infrastructure — ParserContext[T, K]

///|
/// A sorted sequence addressable by integer offset.
/// Implement this trait to make a type searchable with lower_bound.
///
/// Design note: uses a trait instead of a closure pair (count, get_key) so the
/// same lower_bound function works on both Array[OldToken] (searched by .start)
/// and ReuseCursor[T,K] (searched by get_start closure) without threading extra
/// arguments. Two impls suffice — no stdlib binary_search is available in
/// MoonBit, and stdlib exact-match search would require Compare on full elements
/// rather than on an extracted key field.
priv trait OffsetIndexed {
  /// Total number of elements in the sorted sequence.
  length(Self) -> Int
  /// The sort key (code-unit offset) at index i. Must be non-decreasing in i.
  offset_at(Self, Int) -> Int
}

///|
/// Lower-bound binary search over any OffsetIndexed source.
///
/// Returns the first index i where offset_at(i) >= target, or source.length()
/// if every element is smaller (i.e. "would insert at end").
///
/// Invariant maintained throughout:
///   - Every index < lo has offset_at < target  (definitely too small)
///   - Every index >= hi has offset_at >= target (candidate or answer)
/// When lo == hi the loop exits and lo is the answer.
///
/// Note: uses < rather than subtraction for comparison to avoid integer
/// overflow on large offsets.
fn[S : OffsetIndexed] lower_bound(source : S, target : Int) -> Int {
  let mut lo = 0
  let mut hi = source.length()
  while lo < hi {
    let mid = lo + (hi - lo) / 2
    if source.offset_at(mid) < target {
      lo = mid + 1 // mid is too small; exclude it
    } else {
      hi = mid // mid is a candidate; keep it
    }
  }
  lo
}

///|
/// Generic token with source position. T is the language-specific token type.
pub struct TokenInfo[T] {
  token : T
  start : Int // code-unit offset, inclusive
  end : Int // code-unit offset, exclusive
} derive(Show, Eq)

///|
/// A parse diagnostic (error or warning) with source position.
/// Stores the offending token so consumers never need to re-tokenize.
pub struct Diagnostic[T] {
  message : String
  start : Int
  end : Int
  got_token : T
} derive(Show)

///|
/// Lex error raised by a tokenize_fn when the source contains unrecognizable input.
/// Generic replacement for language-specific tokenization error types.
pub suberror LexError {
  LexError(String)
}

///|
/// Describes one language to the generic parser infrastructure.
/// Create one instance at module init — it is reused across all parses.
///
/// Fields:
///   kind_to_raw     — maps your SyntaxKind to the CST's RawKind (an Int)
///   token_is_eof    — returns true for the end-of-input sentinel token
///   token_is_trivia — returns true for whitespace/trivia tokens to skip in peek
///   tokens_equal    — equality check (needed because T has no Eq constraint)
///   print_token     — human-readable representation of T (for diagnostics)
///   whitespace_kind — the SyntaxKind for whitespace trivia nodes in the tree
///   error_kind      — the SyntaxKind for error tokens/nodes
///   root_kind       — the SyntaxKind for the root node (e.g. SourceFile)
///   eof_token       — the T value returned when past end of input
pub struct LanguageSpec[T, K] {
  kind_to_raw : (K) -> @seam.RawKind
  token_is_eof : (T) -> Bool
  token_is_trivia : (T) -> Bool
  tokens_equal : (T, T) -> Bool
  print_token : (T) -> String
  whitespace_kind : K
  error_kind : K
  root_kind : K
  eof_token : T
  // reuse support: classify old-tree RawKind values and match old leaves to new tokens
  raw_is_trivia : (@seam.RawKind) -> Bool
  raw_is_error : (@seam.RawKind) -> Bool
  cst_token_matches : (@seam.RawKind, String, T) -> Bool
}

///|
/// Construct a TokenInfo. Use this from outside the core package.
pub fn[T] TokenInfo::new(token : T, start : Int, end : Int) -> TokenInfo[T] {
  { token, start, end }
}

///|
/// Construct a LanguageSpec. Use this from outside the core package.
pub fn[T, K] LanguageSpec::new(
  kind_to_raw : (K) -> @seam.RawKind,
  token_is_eof : (T) -> Bool,
  token_is_trivia : (T) -> Bool,
  tokens_equal : (T, T) -> Bool,
  print_token : (T) -> String,
  whitespace_kind : K,
  error_kind : K,
  root_kind : K,
  eof_token : T,
  raw_is_trivia? : (@seam.RawKind) -> Bool = fn(_) { false },
  raw_is_error? : (@seam.RawKind) -> Bool = fn(_) { false },
  cst_token_matches? : (@seam.RawKind, String, T) -> Bool = fn(_, _, _) {
    false
  },
) -> LanguageSpec[T, K] {
  {
    kind_to_raw,
    token_is_eof,
    token_is_trivia,
    tokens_equal,
    print_token,
    whitespace_kind,
    error_kind,
    root_kind,
    eof_token,
    raw_is_trivia,
    raw_is_error,
    cst_token_matches,
  }
}

///|
/// Core parser state. Grammar functions receive this and call methods on it.
/// T = token type, K = syntax-kind type.
///
/// Token storage is closure-based: callers provide indexed accessors so
/// ParserContext never needs to copy tokens into a wrapper array.
pub struct ParserContext[T, K] {
  spec : LanguageSpec[T, K]
  token_count : Int
  get_token : (Int) -> T
  get_start : (Int) -> Int
  get_end : (Int) -> Int
  source : String
  mut position : Int
  events : @seam.EventBuffer
  errors : Array[Diagnostic[T]]
  mut error_count : Int
  mut open_nodes : Int
  // Incremental reuse — None on a fresh parse, Some(cursor) for incremental.
  mut reuse_cursor : ReuseCursor[T, K]?
  // Diagnostics from the previous parse (optional). When present, reuse hits
  // replay matching diagnostics by code-unit span to preserve exact messages.
  mut reuse_diagnostics : Array[Diagnostic[T]]?
  mut reuse_count : Int
}

///|
/// Construct a ParserContext from an Array[TokenInfo[T]].
/// Convenience wrapper — internally creates indexed accessors over the array.
pub fn[T, K] ParserContext::new(
  tokens : Array[TokenInfo[T]],
  source : String,
  spec : LanguageSpec[T, K],
) -> ParserContext[T, K] {
  {
    spec,
    token_count: tokens.length(),
    get_token: fn(i) { tokens[i].token },
    get_start: fn(i) { tokens[i].start },
    get_end: fn(i) { tokens[i].end },
    source,
    position: 0,
    events: @seam.EventBuffer::new(),
    errors: [],
    error_count: 0,
    open_nodes: 0,
    reuse_cursor: None,
    reuse_diagnostics: None,
    reuse_count: 0,
  }
}

///|
/// Construct a ParserContext from indexed accessor closures.
/// Use this to avoid allocating a wrapper Array[TokenInfo[T]] when the
/// caller already has tokens in a different layout (e.g. @token.TokenInfo).
pub fn[T, K] ParserContext::new_indexed(
  token_count : Int,
  get_token : (Int) -> T,
  get_start : (Int) -> Int,
  get_end : (Int) -> Int,
  source : String,
  spec : LanguageSpec[T, K],
) -> ParserContext[T, K] {
  {
    spec,
    token_count,
    get_token,
    get_start,
    get_end,
    source,
    position: 0,
    events: @seam.EventBuffer::new(),
    errors: [],
    error_count: 0,
    open_nodes: 0,
    reuse_cursor: None,
    reuse_diagnostics: None,
    reuse_count: 0,
  }
}

///|
/// Return the next non-trivia token without consuming it.
pub fn[T, K] ParserContext::peek(self : ParserContext[T, K]) -> T {
  let mut pos = self.position
  while pos < self.token_count {
    let t = (self.get_token)(pos)
    if (self.spec.token_is_trivia)(t) {
      pos = pos + 1
    } else {
      return t
    }
  }
  self.spec.eof_token
}

///|
/// Return the TokenInfo for the next non-trivia token.
pub fn[T, K] ParserContext::peek_info(
  self : ParserContext[T, K],
) -> TokenInfo[T] {
  let mut pos = self.position
  while pos < self.token_count {
    let token = (self.get_token)(pos)
    if (self.spec.token_is_trivia)(token) {
      pos = pos + 1
    } else {
      return { token, start: (self.get_start)(pos), end: (self.get_end)(pos) }
    }
  }
  let end = self.source.length()
  { token: self.spec.eof_token, start: end, end }
}

///|
/// Return true if the current non-trivia token equals the given token.
pub fn[T, K] ParserContext::at(self : ParserContext[T, K], token : T) -> Bool {
  (self.spec.tokens_equal)(self.peek(), token)
}

///|
/// Return true if at end of input (ignoring trivia).
pub fn[T, K] ParserContext::at_eof(self : ParserContext[T, K]) -> Bool {
  (self.spec.token_is_eof)(self.peek())
}

///|
/// Extract source text for a token from the source string.
/// Returns "" on invalid spans rather than aborting — span errors (whether
/// from a tokenizer bug or a grammar invariant violation) produce an empty
/// leaf in the tree instead of crashing a long-running host process or LSP.
pub fn[T, K] ParserContext::token_text(
  self : ParserContext[T, K],
  info : TokenInfo[T],
) -> String {
  let slice : StringView = self.source[info.start:info.end] catch {
      _ => return ""
    }
  slice.to_string()
}

///|
/// Extract source text at a token index. Internal helper to avoid constructing
/// intermediate TokenInfo structs on the hot path.
/// Returns "" on invalid spans (same policy as token_text).
fn[T, K] ParserContext::text_at(
  self : ParserContext[T, K],
  pos : Int,
) -> String {
  let start = (self.get_start)(pos)
  let end = (self.get_end)(pos)
  let slice : StringView = self.source[start:end] catch { _ => return "" }
  slice.to_string()
}

///|
/// Emit all consecutive trivia tokens at current position to the event stream,
/// advancing position past each one. Call this before emitting syntactic tokens,
/// and at the end of the top-level grammar function before returning.
pub fn[T, K] ParserContext::flush_trivia(self : ParserContext[T, K]) -> Unit {
  while self.position < self.token_count {
    let token = (self.get_token)(self.position)
    if (self.spec.token_is_trivia)(token) {
      let text = self.text_at(self.position)
      self.events.push(
        @seam.ParseEvent::Token(
          (self.spec.kind_to_raw)(self.spec.whitespace_kind),
          text,
        ),
      )
      self.position = self.position + 1
    } else {
      break
    }
  }
}

///|
/// Consume the current token and emit it as a leaf in the CST.
/// Automatically flushes leading trivia before the token.
pub fn[T, K] ParserContext::emit_token(
  self : ParserContext[T, K],
  kind : K,
) -> Unit {
  if self.at_eof() {
    abort(
      "emit_token: called at EOF — grammar tried to consume past end of input",
    )
  }
  self.flush_trivia()
  let text = self.text_at(self.position)
  self.events.push(@seam.ParseEvent::Token((self.spec.kind_to_raw)(kind), text))
  self.position = self.position + 1
}

///|
/// Open a new node. Must be followed by finish_node().
pub fn[T, K] ParserContext::start_node(
  self : ParserContext[T, K],
  kind : K,
) -> Unit {
  self.open_nodes = self.open_nodes + 1
  self.events.push(@seam.StartNode((self.spec.kind_to_raw)(kind)))
}

///|
/// Close the most recently opened node.
pub fn[T, K] ParserContext::finish_node(self : ParserContext[T, K]) -> Unit {
  if self.open_nodes <= 0 {
    abort("finish_node: no matching start_node")
  }
  self.open_nodes = self.open_nodes - 1
  self.events.push(@seam.FinishNode)
}

///|
/// Reserve a placeholder that can later be claimed by start_at.
/// Used for retroactive wrapping (e.g. binary expressions, applications).
pub fn[T, K] ParserContext::mark(self : ParserContext[T, K]) -> Int {
  self.events.mark()
}

///|
/// Retroactively open a node at a previously marked position.
pub fn[T, K] ParserContext::start_at(
  self : ParserContext[T, K],
  mark : Int,
  kind : K,
) -> Unit {
  self.events.start_at(mark, (self.spec.kind_to_raw)(kind))
  self.open_nodes = self.open_nodes + 1
}

///|
/// Record a diagnostic at the current position. Does not consume a token.
/// Captures the offending token so consumers never need to re-tokenize.
/// If an equivalent diagnostic (same message/start/end) already exists, this
/// updates the stored token payload instead of appending a duplicate entry.
pub fn[T, K] ParserContext::error(
  self : ParserContext[T, K],
  msg : String,
) -> Unit {
  let info = self.peek_info()
  let _ = self.push_diagnostic_unique({
    message: msg,
    start: info.start,
    end: info.end,
    got_token: info.token,
  })
}

///|
/// Consume the current token and emit it as an error token (for error recovery).
/// Automatically flushes leading trivia before the token.
pub fn[T, K] ParserContext::bump_error(self : ParserContext[T, K]) -> Unit {
  self.flush_trivia()
  let text = self.text_at(self.position)
  self.events.push(
    @seam.ParseEvent::Token((self.spec.kind_to_raw)(self.spec.error_kind), text),
  )
  self.position = self.position + 1
}

///|
/// Emit a zero-width token (no source text consumed, position unchanged).
/// Use this for synthetic tokens inserted during error recovery when the
/// grammar needs to represent a missing token without consuming input.
pub fn[T, K] ParserContext::emit_zero_width(
  self : ParserContext[T, K],
  kind : K,
) -> Unit {
  self.events.push(@seam.ParseEvent::Token((self.spec.kind_to_raw)(kind), ""))
}

///|
/// Emit a zero-width error placeholder (no source text consumed).
/// Shorthand for emit_zero_width(spec.error_kind). Use this in grammar code
/// to represent a missing or erroneous token without consuming input.
pub fn[T, K] ParserContext::emit_error_placeholder(
  self : ParserContext[T, K],
) -> Unit {
  self.emit_zero_width(self.spec.error_kind)
}

// ─── Incremental reuse ────────────────────────────────────────────────────────

///|
/// Attach a reuse cursor to an existing ParserContext, enabling incremental parsing.
/// Called by entry points (parse_cst_with_cursor etc.) after construction.
pub fn[T, K] ParserContext::set_reuse_cursor(
  self : ParserContext[T, K],
  cursor : ReuseCursor[T, K],
) -> Unit {
  self.reuse_cursor = Some(cursor)
}

///|
/// Attach diagnostics from the previous parse. On reuse hits, diagnostics whose
/// spans fall within the reused node are replayed into self.errors.
pub fn[T, K] ParserContext::set_reuse_diagnostics(
  self : ParserContext[T, K],
  diagnostics : Array[Diagnostic[T]],
) -> Unit {
  self.reuse_diagnostics = Some(diagnostics)
}

///|
/// Try to reuse an old CST node of `kind` at the current parse position.
///
/// byte_offset uses self.position directly (including any leading trivia) because
/// seek_node_at matches nodes by their absolute start, and nodes store trivia as
/// their first children — so the node's start offset is the trivia's start offset.
///
/// token_pos must skip past trivia first. leading_token_matches compares the
/// node's first *non-trivia* token against the token at token_pos in the new
/// stream. Passing a trivia index causes systematic false-negative reuse for
/// all whitespace-separated constructs (the common case).
///
/// Returns None immediately if no cursor is attached (non-incremental parse).
fn[T, K] ParserContext::try_reuse(
  self : ParserContext[T, K],
  kind : K,
) -> @seam.CstNode? {
  if self.position >= self.token_count {
    return None
  }
  let byte_offset = (self.get_start)(self.position)
  // Skip trivia to find the logical token position for leading-token matching.
  let mut token_pos = self.position
  while token_pos < self.token_count &&
        (self.spec.token_is_trivia)((self.get_token)(token_pos)) {
    token_pos = token_pos + 1
  }
  match self.reuse_cursor {
    None => None
    Some(cursor) =>
      cursor.try_reuse((self.spec.kind_to_raw)(kind), byte_offset, token_pos)
  }
}

///|
/// Recursively emit StartNode/Token/FinishNode events for a reused CST node.
/// Pure function — does not advance position. Must be followed by advance_past_reused.
fn[T, K] ParserContext::emit_node_events(
  self : ParserContext[T, K],
  node : @seam.CstNode,
) -> Unit {
  self.events.push(@seam.ParseEvent::StartNode(node.kind))
  for child in node.children {
    match child {
      @seam.CstElement::Token(t) =>
        self.events.push(@seam.ParseEvent::Token(t.kind, t.text))
      @seam.CstElement::Node(n) => self.emit_node_events(n)
    }
  }
  self.events.push(@seam.ParseEvent::FinishNode)
}

///|
/// Advance position past all tokens (trivia + non-trivia) covered by node.
/// Offset-based advancing avoids over-consuming when reused subtrees contain
/// zero-width non-trivia leaves (e.g. error placeholders).
fn[T, K] ParserContext::advance_past_reused(
  self : ParserContext[T, K],
  node : @seam.CstNode,
) -> Unit {
  if self.position >= self.token_count {
    return
  }
  let node_end = (self.get_start)(self.position) + node.text_len
  while self.position < self.token_count &&
        (self.get_start)(self.position) < node_end {
    self.position = self.position + 1
  }
}

///|
priv struct ReusedErrorSpan {
  start : Int
  end : Int
}

///|
/// Collect error spans from a CST subtree.
/// Returns the number of spans added into `out`.
fn[T, K] collect_reused_error_spans(
  node : @seam.CstNode,
  node_start : Int,
  spec : LanguageSpec[T, K],
  out : Array[ReusedErrorSpan],
) -> Int {
  let mut offset = node_start
  let mut added = 0
  for child in node.children {
    match child {
      @seam.CstElement::Token(t) => {
        let end = offset + t.text_len()
        if (spec.raw_is_error)(t.kind) {
          out.push({ start: offset, end })
          added = added + 1
        }
        offset = end
      }
      @seam.CstElement::Node(n) => {
        let child_added = collect_reused_error_spans(n, offset, spec, out)
        if child_added == 0 && (spec.raw_is_error)(n.kind) {
          out.push({ start: offset, end: offset + n.text_len })
          added = added + 1
        } else {
          added = added + child_added
        }
        offset = offset + n.text_len
      }
    }
  }
  added
}

///|
/// Return the first token that starts at or after byte_offset, or overlaps it
/// (end > byte_offset). Overlap handling covers zero-width tokens and error
/// spans that fall mid-token. Scans forward from from_pos.
fn[T, K] ParserContext::token_info_at_or_after(
  self : ParserContext[T, K],
  from_pos : Int,
  byte_offset : Int,
) -> TokenInfo[T] {
  let mut pos = from_pos
  while pos < self.token_count {
    let start = (self.get_start)(pos)
    let end = (self.get_end)(pos)
    if start >= byte_offset || end > byte_offset {
      return { token: (self.get_token)(pos), start, end }
    }
    pos = pos + 1
  }
  let eof = self.source.length()
  { token: self.spec.eof_token, start: eof, end: eof }
}

///|
/// Add a diagnostic unless an equivalent one already exists.
/// Equivalence: same message and span. If found, update got_token to the latest.
/// Returns true if newly added, false if an existing diagnostic was updated.
fn[T, K] ParserContext::push_diagnostic_unique(
  self : ParserContext[T, K],
  diag : Diagnostic[T],
) -> Bool {
  for i, existing in self.errors.iter2() {
    if existing.message == diag.message &&
      existing.start == diag.start &&
      existing.end == diag.end {
      // Keep a single logical diagnostic per message/span, but refresh token
      // with the most recent parse emission.
      self.errors[i] = diag
      return false
    }
  }
  self.errors.push(diag)
  self.error_count = self.error_count + 1
  true
}

///|
/// Replay previous diagnostics that fall fully within [node_start, node_end).
///
/// allow_eof_boundary: pass true when the reused node has no error-bearing
/// next sibling — meaning a zero-width diagnostic at node_end that is also
/// at EOF was emitted by this node's grammar (ctx.error without placeholder).
/// Pass false when the next sibling has error content, meaning the boundary
/// diagnostic belongs to that sibling, not this node.
fn[T, K] ParserContext::replay_reused_diagnostics(
  self : ParserContext[T, K],
  node_start : Int,
  node_end : Int,
  owns_right_boundary_zero_width_error : Bool,
  allow_eof_boundary : Bool,
) -> Bool {
  let mut replayed_any = false
  match self.reuse_diagnostics {
    None => ()
    Some(prev) =>
      for d in prev {
        // Half-open containment: [node_start, node_end), with one exception:
        // keep zero-width diagnostics at node_end only when this reused node
        // itself owns a zero-width error at that boundary (structural check),
        // OR when the node ends at EOF and no error-bearing sibling follows
        // (handles ctx.error at EOF without emit_error_placeholder).
        let keep_right_boundary = d.start == node_end &&
          d.end == node_end &&
          (
            owns_right_boundary_zero_width_error ||
            (allow_eof_boundary && node_end >= self.source.length())
          )
        if d.start >= node_start &&
          d.end <= node_end &&
          (d.start < node_end || keep_right_boundary) {
          if self.push_diagnostic_unique(d) {
            replayed_any = true
          }
        }
      }
  }
  replayed_any
}

///|
/// If previous diagnostics are unavailable, synthesize diagnostics from
/// pre-collected error spans in the reused subtree.
fn[T, K] ParserContext::synthesize_reused_diagnostics(
  self : ParserContext[T, K],
  spans : Array[ReusedErrorSpan],
) -> Unit {
  for span in spans {
    let info = self.token_info_at_or_after(self.position, span.start)
    let _ = self.push_diagnostic_unique({
      message: "reused syntax error",
      start: span.start,
      end: span.end,
      got_token: info.token,
    })
  }
}

///|
/// Emit all events for a reused node, replay its diagnostics, and advance
/// both positions past it. Order matters:
///   1. emit_node_events   — replay events (pure, no position change)
///   2. replay/synthesize  — restore diagnostics while self.position still
///                           points at node start (token_info_at_or_after needs it)
///   3. advance_past_reused — advance self.position through the new token stream
///   4. cursor.advance_past — advance cursor.current_offset through the old tree
///
/// Two separate advance calls because there are two independent positions:
///   advance_past_reused — walks self.position through the *new* token stream
///   cursor.advance_past  — walks cursor.current_offset through the *old* tree
/// Both must stay in sync; without cursor.advance_past, subsequent seeks would
/// detect false backward movement and reset the cursor to the root.
///
/// The None arm is unreachable in practice: emit_reused is only called when
/// try_reuse returns Some, which requires reuse_cursor to be Some. It is
/// handled defensively to keep the match exhaustive.
fn[T, K] ParserContext::emit_reused(
  self : ParserContext[T, K],
  node : @seam.CstNode,
) -> Unit {
  let node_start = if self.position < self.token_count {
    (self.get_start)(self.position)
  } else {
    self.source.length()
  }
  let node_end = node_start + node.text_len
  // Compute error spans once — used for both the boundary check and synthesis.
  let error_spans : Array[ReusedErrorSpan] = []
  let _ = collect_reused_error_spans(node, node_start, self.spec, error_spans)
  let owns_right_boundary = error_spans
    .iter()
    .any(fn(s) { s.start == node_end && s.end == node_end })
  self.emit_node_events(node)
  // allow_eof_boundary: true when no error-bearing sibling follows this node,
  // meaning a zero-width diagnostic at node_end that is at EOF was emitted by
  // this node's own grammar (ctx.error without emit_error_placeholder).
  // When the next sibling has error content it owns that boundary diagnostic.
  let allow_eof_boundary = match self.reuse_cursor {
    None => true
    Some(cursor) => not(cursor.next_sibling_has_error())
  }
  // Diagnostic replay must happen before advance_past_reused: synthesize uses
  // self.position to locate tokens within the node's span.
  if not(
      self.replay_reused_diagnostics(
        node_start, node_end, owns_right_boundary, allow_eof_boundary,
      ),
    ) {
    self.synthesize_reused_diagnostics(error_spans)
  }
  self.advance_past_reused(node)
  match self.reuse_cursor {
    Some(cursor) => cursor.advance_past(node)
    None => () // unreachable — no cursor means try_reuse always returns None
  }
  self.reuse_count = self.reuse_count + 1
}

///|
/// Reuse-aware node combinator. The core grammar API.
/// On reuse hit: emits the old node and skips body entirely — O(1) per node.
/// On miss: start_node → body() → finish_node — standard parse path.
pub fn[T, K] ParserContext::node(
  self : ParserContext[T, K],
  kind : K,
  body : () -> Unit,
) -> Unit {
  if self.try_reuse(kind) is Some(reuse) {
    // Fast path — reuse hit:
    // Replays the old subtree's events and advances position past it.
    // `body` is never called: the closure is allocated but not invoked.
    // reuse_count increments; no start_node/finish_node are pushed.
    self.emit_reused(reuse)
  } else {
    // Slow path — cache miss:
    // Runs the grammar body and wraps its output in a new node.
    // Every token consumed inside body() advances position normally.
    // Errors recorded inside body() accumulate in self.errors as usual.
    self.start_node(kind)
    body()
    self.finish_node()
  }
}

///|
/// Retroactive node wrapper for expressions built via mark()/start_at().
/// Reuse is not attempted here — the prefix is already emitted.
/// Inner node() calls within body() still benefit from reuse.
pub fn[T, K] ParserContext::wrap_at(
  self : ParserContext[T, K],
  mark : Int,
  kind : K,
  body : () -> Unit,
) -> Unit {
  self.start_at(mark, kind)
  body()
  self.finish_node()
}

// ─── Tests ───────────────────────────────────────────────────────────────────

///|
test "TokenInfo stores token with position" {
  let info : TokenInfo[String] = { token: "hello", start: 0, end: 5 }
  inspect(info.start, content="0")
  inspect(info.end, content="5")
  inspect(info.token, content="hello")
}

///|
test "Diagnostic stores message, position, and token" {
  let d : Diagnostic[String] = {
    message: "unexpected token",
    start: 3,
    end: 7,
    got_token: "bad",
  }
  inspect(d.message, content="unexpected token")
  inspect(d.got_token, content="bad")
}

///|
fn make_test_fixtures() -> (LanguageSpec[String, Int], Array[TokenInfo[String]]) {
  let spec : LanguageSpec[String, Int] = {
    kind_to_raw: fn(k) { @seam.RawKind(k) },
    token_is_eof: fn(t) { t == "EOF" },
    token_is_trivia: fn(_) { false },
    tokens_equal: fn(a, b) { a == b },
    print_token: fn(t) { t },
    whitespace_kind: 0,
    error_kind: 1,
    root_kind: 2,
    eof_token: "EOF",
    raw_is_trivia: fn(_) { false },
    raw_is_error: fn(_) { false },
    cst_token_matches: fn(_, _, _) { false },
  }
  let tokens = [
    { token: "a", start: 0, end: 1 },
    { token: "b", start: 1, end: 2 },
  ]
  (spec, tokens)
}

///|
test "ParserContext can be constructed" {
  let (spec, tokens) = make_test_fixtures()
  let ctx = ParserContext::new(tokens, "ab", spec)
  inspect(ctx.position, content="0")
}

///|
test "peek returns current token" {
  let (spec, tokens) = make_test_fixtures()
  let ctx = ParserContext::new(tokens, "ab", spec)
  inspect(ctx.peek(), content="a")
}

///|
test "peek returns eof when past end" {
  let (spec, _) = make_test_fixtures()
  let ctx = ParserContext::new([], "", spec)
  inspect(ctx.peek(), content="EOF")
}

///|
test "peek skips trivia tokens" {
  let spec : LanguageSpec[String, Int] = {
    kind_to_raw: fn(k) { @seam.RawKind(k) },
    token_is_eof: fn(t) { t == "EOF" },
    token_is_trivia: fn(t) { t == " " },
    tokens_equal: fn(a, b) { a == b },
    print_token: fn(t) { t },
    whitespace_kind: 0,
    error_kind: 1,
    root_kind: 2,
    eof_token: "EOF",
    raw_is_trivia: fn(_) { false },
    raw_is_error: fn(_) { false },
    cst_token_matches: fn(_, _, _) { false },
  }
  let tokens = [
    { token: " ", start: 0, end: 1 },
    { token: " ", start: 1, end: 2 },
    { token: "x", start: 2, end: 3 },
  ]
  let ctx = ParserContext::new(tokens, "  x", spec)
  inspect(ctx.peek(), content="x")
}

///|
test "at matches current token" {
  let (spec, tokens) = make_test_fixtures()
  let ctx = ParserContext::new(tokens, "ab", spec)
  inspect(ctx.at("a"), content="true")
  inspect(ctx.at("b"), content="false")
}

// ─── parse_with ───────────────────────────────────────────────────────────────

///|
/// Parse a source string using the given language spec and grammar function.
/// Returns the immutable CST and any parse diagnostics.
///
/// tokenize — converts source to a flat token array (including trivia tokens)
/// grammar  — the entry-point parse function; calls ctx methods to build the tree
pub fn[T, K] parse_with(
  source : String,
  spec : LanguageSpec[T, K],
  tokenize : (String) -> Array[TokenInfo[T]],
  grammar : (ParserContext[T, K]) -> Unit,
) -> (@seam.CstNode, Array[Diagnostic[T]]) {
  let tokens = tokenize(source)
  let ctx = ParserContext::new(tokens, source, spec)
  grammar(ctx)
  ctx.flush_trivia()
  if ctx.open_nodes != 0 {
    abort(
      "parse_with: grammar left " +
      ctx.open_nodes.to_string() +
      " unclosed nodes",
    )
  }
  let tree = ctx.events.build_tree(
    (spec.kind_to_raw)(spec.root_kind),
    trivia_kind=Some((spec.kind_to_raw)(spec.whitespace_kind)),
  )
  (tree, ctx.errors)
}
